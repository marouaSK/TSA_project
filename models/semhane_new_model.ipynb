{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5efe345f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Data loaded.\n",
      "Extracting statistical features...\n",
      "Extracting FFT features...\n",
      "Extracting Wavelet features...\n",
      "Skipping ARIMA feature extraction.\n",
      "Skipping Prophet feature extraction.\n",
      "Shape of engineered train features: (3999, 92)\n",
      "\n",
      "Training Logistic Regression on engineered features...\n",
      "Training Random Forest on engineered features (with GridSearchCV)...\n",
      "Fitting 3 folds for each of 72 candidates, totalling 216 fits\n",
      "Best RF params: {'class_weight': 'balanced_subsample', 'max_depth': 20, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 200}\n",
      "Training SVM on engineered features...\n",
      "Training Decision Tree on engineered features...\n",
      "Training KNN on engineered features...\n",
      "Training AdaBoost on engineered features...\n",
      "Training XGBoost on engineered features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Semhane\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\training.py:183: UserWarning: [02:46:19] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training TimeSeries Forest on scaled TS data...\n",
      "\n",
      "Validation Accuracies (Traditional Models & TSF):\n",
      "Logistic Regression (Eng. Feat.): 0.6370\n",
      "Random Forest (Eng. Feat.): 0.6880\n",
      "SVM (Eng. Feat.): 0.6650\n",
      "Decision Tree (Eng. Feat.): 0.5730\n",
      "KNN (Eng. Feat.): 0.6300\n",
      "AdaBoost (Eng. Feat.): 0.5450\n",
      "XGBoost (Eng. Feat.): 0.7010\n",
      "TimeSeries Forest (Raw TS): 0.5660\n",
      "\n",
      "Using CPU for PyTorch\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1              [-1, 32, 178]             256\n",
      "       BatchNorm1d-2              [-1, 32, 178]              64\n",
      "              ReLU-3              [-1, 32, 178]               0\n",
      "         MaxPool1d-4               [-1, 32, 89]               0\n",
      "            Conv1d-5               [-1, 64, 89]          14,400\n",
      "       BatchNorm1d-6               [-1, 64, 89]             128\n",
      "              ReLU-7               [-1, 64, 89]               0\n",
      "         MaxPool1d-8               [-1, 64, 44]               0\n",
      "            Conv1d-9              [-1, 128, 44]          57,472\n",
      "      BatchNorm1d-10              [-1, 128, 44]             256\n",
      "             ReLU-11              [-1, 128, 44]               0\n",
      "        MaxPool1d-12              [-1, 128, 22]               0\n",
      "          Flatten-13                 [-1, 2816]               0\n",
      "          Dropout-14                 [-1, 2816]               0\n",
      "           Linear-15                  [-1, 256]         721,152\n",
      "             ReLU-16                  [-1, 256]               0\n",
      "           Linear-17                    [-1, 5]           1,285\n",
      "================================================================\n",
      "Total params: 795,013\n",
      "Trainable params: 795,013\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.50\n",
      "Params size (MB): 3.03\n",
      "Estimated Total Size (MB): 3.53\n",
      "----------------------------------------------------------------\n",
      "None\n",
      "\n",
      "Training 1D-CNN...\n",
      "Epoch 1/60, Train Loss: 1.1639, Val Loss: 0.9786, Val Acc: 0.5920\n",
      "New best CNN val acc: 0.5920. Model saved.\n",
      "Epoch 2/60, Train Loss: 0.9454, Val Loss: 0.9411, Val Acc: 0.6180\n",
      "New best CNN val acc: 0.6180. Model saved.\n",
      "Epoch 3/60, Train Loss: 0.8510, Val Loss: 0.8005, Val Acc: 0.6780\n",
      "New best CNN val acc: 0.6780. Model saved.\n",
      "Epoch 4/60, Train Loss: 0.7720, Val Loss: 0.7916, Val Acc: 0.6750\n",
      "Epoch 5/60, Train Loss: 0.7143, Val Loss: 0.8476, Val Acc: 0.6630\n",
      "Epoch 6/60, Train Loss: 0.6804, Val Loss: 0.7496, Val Acc: 0.7150\n",
      "New best CNN val acc: 0.7150. Model saved.\n",
      "Epoch 7/60, Train Loss: 0.6337, Val Loss: 0.7350, Val Acc: 0.7180\n",
      "New best CNN val acc: 0.7180. Model saved.\n",
      "Epoch 8/60, Train Loss: 0.5874, Val Loss: 0.7093, Val Acc: 0.7130\n",
      "Epoch 9/60, Train Loss: 0.5571, Val Loss: 0.7152, Val Acc: 0.7170\n",
      "Epoch 10/60, Train Loss: 0.5340, Val Loss: 0.7532, Val Acc: 0.6900\n",
      "Epoch 11/60, Train Loss: 0.4978, Val Loss: 0.8146, Val Acc: 0.7000\n",
      "Epoch 12/60, Train Loss: 0.4653, Val Loss: 0.7840, Val Acc: 0.7030\n",
      "Epoch 13/60, Train Loss: 0.4438, Val Loss: 0.7703, Val Acc: 0.6960\n",
      "Epoch 14/60, Train Loss: 0.4224, Val Loss: 0.7854, Val Acc: 0.6890\n",
      "Epoch 15/60, Train Loss: 0.3418, Val Loss: 0.8159, Val Acc: 0.7140\n",
      "Epoch 16/60, Train Loss: 0.3226, Val Loss: 0.8596, Val Acc: 0.7140\n",
      "Epoch 17/60, Train Loss: 0.3056, Val Loss: 0.8168, Val Acc: 0.7020\n",
      "Epoch 18/60, Train Loss: 0.2837, Val Loss: 0.8785, Val Acc: 0.6860\n",
      "Epoch 19/60, Train Loss: 0.2636, Val Loss: 0.8578, Val Acc: 0.6970\n",
      "Epoch 20/60, Train Loss: 0.2667, Val Loss: 0.8467, Val Acc: 0.7190\n",
      "New best CNN val acc: 0.7190. Model saved.\n",
      "Epoch 21/60, Train Loss: 0.2288, Val Loss: 0.8353, Val Acc: 0.7110\n",
      "Epoch 22/60, Train Loss: 0.2058, Val Loss: 0.8602, Val Acc: 0.7050\n",
      "Epoch 23/60, Train Loss: 0.2040, Val Loss: 0.8883, Val Acc: 0.7010\n",
      "Epoch 24/60, Train Loss: 0.2038, Val Loss: 0.8755, Val Acc: 0.7060\n",
      "Epoch 25/60, Train Loss: 0.1896, Val Loss: 0.8885, Val Acc: 0.7090\n",
      "Epoch 26/60, Train Loss: 0.1898, Val Loss: 0.8774, Val Acc: 0.6970\n",
      "Epoch 27/60, Train Loss: 0.1826, Val Loss: 0.8978, Val Acc: 0.7150\n",
      "Epoch 28/60, Train Loss: 0.1656, Val Loss: 0.8890, Val Acc: 0.7190\n",
      "Epoch 29/60, Train Loss: 0.1636, Val Loss: 0.8888, Val Acc: 0.7090\n",
      "Epoch 30/60, Train Loss: 0.1606, Val Loss: 0.9186, Val Acc: 0.7150\n",
      "Epoch 31/60, Train Loss: 0.1629, Val Loss: 0.9102, Val Acc: 0.7030\n",
      "Epoch 32/60, Train Loss: 0.1543, Val Loss: 0.9196, Val Acc: 0.7160\n",
      "Epoch 33/60, Train Loss: 0.1570, Val Loss: 0.9286, Val Acc: 0.7070\n",
      "Epoch 34/60, Train Loss: 0.1354, Val Loss: 0.9157, Val Acc: 0.7120\n",
      "Epoch 35/60, Train Loss: 0.1379, Val Loss: 0.9171, Val Acc: 0.7090\n",
      "CNN Early stopping at epoch 35.\n",
      "Loading best CNN model with val acc: 0.7190\n",
      "Best 1D-CNN Validation Accuracy after training: 0.7190\n",
      "\n",
      "Predicting on test set and saving results...\n",
      "Saved: logistic_regression_eng_feat_predictions.csv\n",
      "Saved: random_forest_eng_feat_predictions.csv\n",
      "Saved: svm_eng_feat_predictions.csv\n",
      "Saved: decision_tree_eng_feat_predictions.csv\n",
      "Saved: knn_eng_feat_predictions.csv\n",
      "Saved: adaboost_eng_feat_predictions.csv\n",
      "Saved: xgboost_eng_feat_predictions.csv\n",
      "Saved: timeseries_forest_raw_ts_predictions.csv\n",
      "Saved: 1d-cnn_raw_ts_predictions.csv\n",
      "\n",
      "--- Final Validation Accuracies Summary ---\n",
      "1D-CNN (Raw TS): 0.7190\n",
      "XGBoost (Eng. Feat.): 0.7010\n",
      "Random Forest (Eng. Feat.): 0.6880\n",
      "SVM (Eng. Feat.): 0.6650\n",
      "Logistic Regression (Eng. Feat.): 0.6370\n",
      "KNN (Eng. Feat.): 0.6300\n",
      "Decision Tree (Eng. Feat.): 0.5730\n",
      "TimeSeries Forest (Raw TS): 0.5660\n",
      "AdaBoost (Eng. Feat.): 0.5450\n",
      "\n",
      "Best performing model on validation set: 1D-CNN (Raw TS) with accuracy: 0.7190\n",
      "\n",
      "All operations complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV # GridSearchCV added back\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "# New models to import\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier # Added XGBoost\n",
    "\n",
    "# Existing models for time series\n",
    "from sktime.classification.interval_based import TimeSeriesForestClassifier\n",
    "from sktime.datatypes._panel._convert import from_2d_array_to_nested\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchsummary import summary # Make sure this is installed: pip install torchsummary\n",
    "\n",
    "# Feature extraction imports\n",
    "from scipy.fftpack import fft\n",
    "import pywt\n",
    "import pmdarima as pm # For auto_arima\n",
    "from prophet import Prophet # For Prophet\n",
    "#from prophet.utilities import stan_init # For Prophet with PyStan 3+\n",
    "import logging # To suppress Prophet logs\n",
    "\n",
    "# Suppress excessive logging from Prophet and CmdStanPy\n",
    "logging.getLogger('prophet').setLevel(logging.ERROR)\n",
    "logging.getLogger('cmdstanpy').setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "#==== Load data ====\n",
    "print(\"Loading data...\")\n",
    "train_df = pd.read_csv('Sleep Train 5000.csv', header=None)\n",
    "test_df = pd.read_csv('Sleep Test 1000.csv', header=None)\n",
    "print(\"Data loaded.\")\n",
    "\n",
    "#==== EDA and Visualization (using full raw training data before splits) ====\n",
    "\n",
    "X_train_full_viz = train_df.iloc[:, 1:].values\n",
    "y_train_full_viz = train_df.iloc[:, 0].values\n",
    "def explore_data(df, name=\"Dataset\"): ...\n",
    "train_classes_counts = explore_data(train_df, \"Training Data\")\n",
    "def plot_examples(X, y, n_examples=2): ...\n",
    "plot_examples(X_train_full_viz, y_train_full_viz)\n",
    "\n",
    "# --- Custom Feature Extraction Functions ---\n",
    "def extract_stats(X_data):\n",
    "    features = []\n",
    "    for signal in X_data:\n",
    "        mean = np.mean(signal); std = np.std(signal); var = np.var(signal)\n",
    "        mini = np.min(signal); maxi = np.max(signal); rng = maxi - mini\n",
    "        median = np.median(signal); skew = 0; kurtosis = 0\n",
    "        if std > 1e-6: # Avoid division by zero for flat signals\n",
    "            skew = np.mean(((signal - mean) / std) ** 3)\n",
    "            kurtosis = np.mean(((signal - mean) / std) ** 4) - 3\n",
    "        else: # Handle flat signals\n",
    "            skew = 0\n",
    "            kurtosis = -3 # Kurtosis of a constant is -3 (excess kurtosis)\n",
    "            \n",
    "        zero_crossings = np.sum(np.diff(np.signbit(signal - np.mean(signal))))\n",
    "        # Peak finding can be sensitive; this is a simple version\n",
    "        diff_signal = np.diff(signal)\n",
    "        peaks = np.sum(np.diff(np.signbit(diff_signal)) < 0  & (diff_signal[:-1] > 0)) # Ensure it's a peak\n",
    "\n",
    "        diffs = np.diff(signal)\n",
    "        mean_diff = np.mean(diffs) if len(diffs) > 0 else 0\n",
    "        std_diff = np.std(diffs) if len(diffs) > 0 else 0\n",
    "        energy = np.sum(signal ** 2); power = energy / len(signal) if len(signal) > 0 else 0\n",
    "        q25 = np.percentile(signal, 25); q75 = np.percentile(signal, 75); iqr = q75 - q25\n",
    "        feat = [mean, std, var, mini, maxi, rng, median, skew, kurtosis,\n",
    "                zero_crossings, peaks, mean_diff, std_diff, energy, power, q25, q75, iqr]\n",
    "        features.append(feat)\n",
    "    return np.array(features)\n",
    "\n",
    "def extract_fft_features(X_data, n_coeffs=50):\n",
    "    features = []\n",
    "    for signal in X_data:\n",
    "        fft_values_complex = fft(signal)\n",
    "        fft_values = np.abs(fft_values_complex)\n",
    "        \n",
    "        # Ensure n_coeffs is not larger than half the FFT length\n",
    "        eff_n_coeffs = min(n_coeffs, len(fft_values) // 2)\n",
    "        dominant_freqs = fft_values[:eff_n_coeffs]\n",
    "        if len(dominant_freqs) < n_coeffs: # Pad if signal was too short\n",
    "            dominant_freqs = np.pad(dominant_freqs, (0, n_coeffs - len(dominant_freqs)), 'constant')\n",
    "\n",
    "        total_power = np.sum(fft_values ** 2)\n",
    "        total_power = total_power if total_power > 1e-6 else 1.0 # Avoid division by zero\n",
    "\n",
    "        n_bands = 5\n",
    "        # Ensure band_size is at least 1\n",
    "        band_size = max(1, len(fft_values) // (2 * n_bands)) # Use only first half of FFT spectrum for bands\n",
    "        band_powers = []\n",
    "        for i in range(n_bands):\n",
    "            start = i * band_size\n",
    "            end = min((i + 1) * band_size, len(fft_values) // 2)\n",
    "            if start >= end: # if signal too short, band might be empty\n",
    "                band_powers.append(0)\n",
    "                continue\n",
    "            band_power = np.sum(fft_values[start:end] ** 2) / total_power\n",
    "            band_powers.append(band_power)\n",
    "        \n",
    "        fft_mean = np.mean(fft_values[:len(fft_values)//2]) # Mean of magnitude spectrum (positive freqs)\n",
    "        fft_std = np.std(fft_values[:len(fft_values)//2])\n",
    "        fft_max = np.max(fft_values[:len(fft_values)//2])\n",
    "        fft_peak_idx = np.argmax(fft_values[:len(fft_values)//2]) if len(fft_values) > 0 else 0\n",
    "        \n",
    "        feat = np.concatenate([dominant_freqs, band_powers, [fft_mean, fft_std, fft_max, fft_peak_idx]])\n",
    "        features.append(feat)\n",
    "    return np.array(features)\n",
    "\n",
    "def extract_wavelet_features(X_data, wavelet='db4', level=4):\n",
    "    features = []\n",
    "    # Calculate expected length once\n",
    "    # wavedec returns level+1 arrays of coeffs. For each, 3 stats (mean, std, energy).\n",
    "    expected_feature_length = (level + 1) * 3\n",
    "\n",
    "    for signal in X_data:\n",
    "        wavelet_features_single = []\n",
    "        try:\n",
    "            # Ensure signal is long enough for the decomposition level\n",
    "            min_len = pywt.dwt_max_level(len(signal), pywt.Wavelet(wavelet))\n",
    "            actual_level = min(level, min_len) # Adjust level if signal is too short\n",
    "            \n",
    "            if actual_level < 1 : # Cannot decompose\n",
    "                 coeffs = [] # or handle as an error / default features\n",
    "            else:\n",
    "                coeffs = pywt.wavedec(signal, wavelet, level=actual_level)\n",
    "\n",
    "            for i in range(level + 1): # Iterate up to the original requested level\n",
    "                if i < len(coeffs) and len(coeffs[i]) > 0:\n",
    "                    coef = coeffs[i]\n",
    "                    wavelet_features_single.extend([np.mean(coef), np.std(coef), np.sum(coef ** 2)])\n",
    "                else:\n",
    "                    # If coefficient array is missing (due to adjusted level) or empty, pad with zeros\n",
    "                    wavelet_features_single.extend([0.0, 0.0, 0.0])\n",
    "        \n",
    "        except ValueError: # Catch other pywt errors\n",
    "            wavelet_features_single = [0.0] * expected_feature_length\n",
    "        \n",
    "        # Ensure the feature vector has the expected length\n",
    "        if len(wavelet_features_single) < expected_feature_length:\n",
    "            wavelet_features_single.extend([0.0] * (expected_feature_length - len(wavelet_features_single)))\n",
    "        elif len(wavelet_features_single) > expected_feature_length:\n",
    "            wavelet_features_single = wavelet_features_single[:expected_feature_length]\n",
    "            \n",
    "        features.append(wavelet_features_single)\n",
    "    return np.array(features)\n",
    "\n",
    "def extract_arima_features(X_data):\n",
    "    print(\"Extracting ARIMA features (this may take a very long time)...\")\n",
    "    features = []\n",
    "    # Define a fixed number of features to extract, e.g., p, d, q, aic, bic, sigma2\n",
    "    # If seasonal: P, D, Q, S. For non-seasonal: 3 params for order, 3 for seasonal_order (0,0,0,0), AIC, BIC, sigma2\n",
    "    num_arima_feats = 3 + 1 + 1 + 1 # p,d,q, aic, bic, sigma2\n",
    "    \n",
    "    for i, signal in enumerate(X_data):\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"  Processed {i+1}/{len(X_data)} signals for ARIMA features.\")\n",
    "        try:\n",
    "            # auto_arima can be very slow.\n",
    "            # Consider simplifying: stepwise=True, trace=False, error_action='ignore',\n",
    "            # suppress_warnings=True, seasonal=False (if no strong seasonality expected)\n",
    "            model = pm.auto_arima(signal,\n",
    "                                  start_p=1, start_q=1,\n",
    "                                  max_p=3, max_q=3, # Keep p,q low to speed up\n",
    "                                  d=None,           # Let auto_arima find d\n",
    "                                  seasonal=False,   # Assuming non-seasonal for general segments\n",
    "                                  stepwise=True,    # Faster\n",
    "                                  suppress_warnings=True,\n",
    "                                  error_action='ignore', # Ignore errors and return a simpler model\n",
    "                                  trace=False)\n",
    "            \n",
    "            if model is not None and hasattr(model, 'order'):\n",
    "                p, d, q = model.order\n",
    "                aic = model.aic() if hasattr(model, 'aic') else np.nan\n",
    "                bic = model.bic() if hasattr(model, 'bic') else np.nan\n",
    "                sigma2 = model.params().get('sigma2', np.nan) # Get sigma2 if available\n",
    "                feat = [p, d, q, aic, bic, sigma2]\n",
    "            else: # Model fitting failed or returned None\n",
    "                feat = [np.nan] * num_arima_feats\n",
    "\n",
    "        except Exception as e: # Catch any other errors during ARIMA fitting\n",
    "            # print(f\"ARIMA error for signal {i}: {e}\")\n",
    "            feat = [np.nan] * num_arima_feats\n",
    "        features.append(feat)\n",
    "    \n",
    "    arima_features_array = np.array(features)\n",
    "    # Impute NaNs that may have arisen from fitting issues or missing attributes\n",
    "    # Using column means for imputation; ensure there's at least one valid value per column\n",
    "    if arima_features_array.shape[0] > 0:\n",
    "        col_means = np.nanmean(arima_features_array, axis=0)\n",
    "        # If a whole column is NaN, nanmean returns NaN. Replace these with 0.\n",
    "        col_means = np.where(np.isnan(col_means), 0, col_means)\n",
    "        for j in range(arima_features_array.shape[1]):\n",
    "            col_data = arima_features_array[:, j]\n",
    "            arima_features_array[np.isnan(col_data), j] = col_means[j]\n",
    "            \n",
    "    return arima_features_array\n",
    "\n",
    "\n",
    "def extract_prophet_features(X_data):\n",
    "    print(\"Extracting Prophet features (this may also take a long time)...\")\n",
    "    features = []\n",
    "    # Define fixed features: trend slope, number of changepoints, maybe forecast error std\n",
    "    num_prophet_feats = 3 # Example: growth rate, n_changepoints, trend_std_error\n",
    "    \n",
    "    for i, signal in enumerate(X_data):\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"  Processed {i+1}/{len(X_data)} signals for Prophet features.\")\n",
    "        \n",
    "        df_prophet = pd.DataFrame({\n",
    "            'ds': pd.to_datetime(pd.RangeIndex(start=0, stop=len(signal), step=1), unit='s'),\n",
    "            'y': signal\n",
    "        })\n",
    "        \n",
    "        try:\n",
    "            # Suppress Stan initialization messages\n",
    "            m = Prophet(yearly_seasonality=False, weekly_seasonality=False, daily_seasonality=False,\n",
    "                        # uncertainty_samples=False, # Faster, but no error bands for trend\n",
    "                        changepoint_range=0.8) # Default\n",
    "            \n",
    "            # For PyStan 3+, Prophet might need explicit stan_init\n",
    "            # m.stan_backend.stan_fit = stan_init(m.model) # Uncomment if you see Stan init issues\n",
    "\n",
    "            m.fit(df_prophet, iter=500) # Reduce iterations for speed if needed\n",
    "\n",
    "            # Extract features\n",
    "            growth_rate = m.params['delta'][0][0] if 'delta' in m.params and m.params['delta'].size > 0 else 0.0\n",
    "            num_changepoints = len(m.changepoints)\n",
    "            \n",
    "            # Trend standard error (if uncertainty_samples > 0 or was True)\n",
    "            # This requires forecast to be made.\n",
    "            # future = m.make_future_dataframe(periods=0)\n",
    "            # forecast = m.predict(future)\n",
    "            # trend_std_error = forecast['trend_upper'].iloc[-1] - forecast['trend_lower'].iloc[-1] if 'trend_upper' in forecast else 0.0\n",
    "            # For simplicity now, let's use a placeholder for the third feature.\n",
    "            # One option is the initial trend value (offset k)\n",
    "            initial_trend = m.params['k'][0][0] if 'k' in m.params and m.params['k'].size > 0 else 0.0\n",
    "\n",
    "            feat = [growth_rate, num_changepoints, initial_trend]\n",
    "\n",
    "        except Exception as e:\n",
    "            # print(f\"Prophet error for signal {i}: {e}\")\n",
    "            feat = [0.0] * num_prophet_feats # Default values on error\n",
    "        features.append(feat)\n",
    "    return np.array(features)\n",
    "\n",
    "\n",
    "#==== Data Preparation ====\n",
    "X_all_train_raw = train_df.iloc[:, 1:].values\n",
    "y_all_train_raw = train_df.iloc[:, 0].values\n",
    "\n",
    "# Ensure test data has consistent number of features as training data before feature extraction\n",
    "# (assuming test_df contains only features, no label column)\n",
    "X_test_full_raw = test_df.values\n",
    "if X_test_full_raw.shape[1] < X_all_train_raw.shape[1]:\n",
    "    print(f\"Padding test data from {X_test_full_raw.shape[1]} to {X_all_train_raw.shape[1]} features.\")\n",
    "    padding = np.zeros((X_test_full_raw.shape[0], X_all_train_raw.shape[1] - X_test_full_raw.shape[1]))\n",
    "    X_test_full_raw = np.hstack((X_test_full_raw, padding))\n",
    "elif X_test_full_raw.shape[1] > X_all_train_raw.shape[1]:\n",
    "    print(f\"Truncating test data from {X_test_full_raw.shape[1]} to {X_all_train_raw.shape[1]} features.\")\n",
    "    X_test_full_raw = X_test_full_raw[:, :X_all_train_raw.shape[1]]\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_all_train_encoded = le.fit_transform(y_all_train_raw)\n",
    "num_classes = len(le.classes_)\n",
    "\n",
    "X_train_raw, X_val_raw, y_train, y_val = train_test_split(\n",
    "    X_all_train_raw, y_all_train_encoded, test_size=0.2, random_state=42, stratify=y_all_train_encoded\n",
    ")\n",
    "\n",
    "# --- Feature Engineering ---\n",
    "# Option to skip slow features for faster iteration:\n",
    "RUN_ARIMA_FEATURES = False # SET TO TRUE TO RUN (VERY SLOW)\n",
    "RUN_PROPHET_FEATURES = False # SET TO TRUE TO RUN (VERY SLOW)\n",
    "\n",
    "print(\"Extracting statistical features...\")\n",
    "X_train_stats = extract_stats(X_train_raw)\n",
    "X_val_stats = extract_stats(X_val_raw)\n",
    "X_test_stats = extract_stats(X_test_full_raw)\n",
    "\n",
    "print(\"Extracting FFT features...\")\n",
    "X_train_fft = extract_fft_features(X_train_raw)\n",
    "X_val_fft = extract_fft_features(X_val_raw)\n",
    "X_test_fft = extract_fft_features(X_test_full_raw)\n",
    "\n",
    "print(\"Extracting Wavelet features...\")\n",
    "X_train_wave = extract_wavelet_features(X_train_raw)\n",
    "X_val_wave = extract_wavelet_features(X_val_raw)\n",
    "X_test_wave = extract_wavelet_features(X_test_full_raw)\n",
    "\n",
    "# Initialize lists for feature sets\n",
    "train_feature_sets = [X_train_stats, X_train_fft, X_train_wave]\n",
    "val_feature_sets = [X_val_stats, X_val_fft, X_val_wave]\n",
    "test_feature_sets = [X_test_stats, X_test_fft, X_test_wave]\n",
    "\n",
    "if RUN_ARIMA_FEATURES:\n",
    "    print(\"Extracting ARIMA features for TRAIN data...\")\n",
    "    X_train_arima = extract_arima_features(X_train_raw)\n",
    "    train_feature_sets.append(X_train_arima)\n",
    "    \n",
    "    print(\"Extracting ARIMA features for VALIDATION data...\")\n",
    "    X_val_arima = extract_arima_features(X_val_raw)\n",
    "    val_feature_sets.append(X_val_arima)\n",
    "\n",
    "    print(\"Extracting ARIMA features for TEST data...\")\n",
    "    X_test_arima = extract_arima_features(X_test_full_raw)\n",
    "    test_feature_sets.append(X_test_arima)\n",
    "else:\n",
    "    print(\"Skipping ARIMA feature extraction.\")\n",
    "\n",
    "if RUN_PROPHET_FEATURES:\n",
    "    print(\"Extracting Prophet features for TRAIN data...\")\n",
    "    X_train_prophet = extract_prophet_features(X_train_raw)\n",
    "    train_feature_sets.append(X_train_prophet)\n",
    "\n",
    "    print(\"Extracting Prophet features for VALIDATION data...\")\n",
    "    X_val_prophet = extract_prophet_features(X_val_raw)\n",
    "    val_feature_sets.append(X_val_prophet)\n",
    "\n",
    "    print(\"Extracting Prophet features for TEST data...\")\n",
    "    X_test_prophet = extract_prophet_features(X_test_full_raw)\n",
    "    test_feature_sets.append(X_test_prophet)\n",
    "else:\n",
    "    print(\"Skipping Prophet feature extraction.\")\n",
    "\n",
    "X_train_engineered = np.hstack(train_feature_sets)\n",
    "X_val_engineered = np.hstack(val_feature_sets)\n",
    "X_test_engineered = np.hstack(test_feature_sets)\n",
    "\n",
    "scaler_eng = StandardScaler()\n",
    "X_train_eng_scaled = scaler_eng.fit_transform(X_train_engineered)\n",
    "X_val_eng_scaled = scaler_eng.transform(X_val_engineered)\n",
    "X_test_eng_scaled = scaler_eng.transform(X_test_engineered)\n",
    "print(f\"Shape of engineered train features: {X_train_eng_scaled.shape}\")\n",
    "\n",
    "# Prepare data for sktime and PyTorch (raw time series)\n",
    "scaler_ts = StandardScaler() # Use a separate scaler for raw time series\n",
    "X_train_ts_scaled = scaler_ts.fit_transform(X_train_raw)\n",
    "X_val_ts_scaled = scaler_ts.transform(X_val_raw)\n",
    "X_test_ts_scaled = scaler_ts.transform(X_test_full_raw)\n",
    "\n",
    "X_train_nested = from_2d_array_to_nested(X_train_ts_scaled)\n",
    "X_val_nested = from_2d_array_to_nested(X_val_ts_scaled)\n",
    "X_test_nested = from_2d_array_to_nested(X_test_ts_scaled)\n",
    "\n",
    "#==== Train Models ====\n",
    "models = {}\n",
    "val_accuracies = {}\n",
    "\n",
    "# --- Traditional Models using Engineered Features ---\n",
    "print(\"\\nTraining Logistic Regression on engineered features...\")\n",
    "log_reg = LogisticRegression(random_state=42, max_iter=2000, solver='liblinear', class_weight='balanced')\n",
    "log_reg.fit(X_train_eng_scaled, y_train)\n",
    "models['Logistic Regression (Eng. Feat.)'] = log_reg\n",
    "\n",
    "print(\"Training Random Forest on engineered features (with GridSearchCV)...\")\n",
    "# Example of GridSearchCV for RandomForest\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 150, 200],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'class_weight': ['balanced', 'balanced_subsample']\n",
    "}\n",
    "# Reduce CV folds for speed during development if needed, e.g., cv=3\n",
    "# n_jobs=-1 uses all available cores\n",
    "rf_grid = GridSearchCV(RandomForestClassifier(random_state=42, n_jobs=-1), \n",
    "                       param_grid_rf, cv=3, scoring='accuracy', verbose=1)\n",
    "rf_grid.fit(X_train_eng_scaled, y_train)\n",
    "print(f\"Best RF params: {rf_grid.best_params_}\")\n",
    "rf_clf = rf_grid.best_estimator_\n",
    "models['Random Forest (Eng. Feat.)'] = rf_clf\n",
    "\n",
    "\n",
    "print(\"Training SVM on engineered features...\")\n",
    "# SVM can be slow. Consider GridSearchCV with a smaller parameter space or fewer C values.\n",
    "# For now, using your previous parameters with class_weight.\n",
    "svm_clf = SVC(random_state=42, C=1.0, kernel='rbf', class_weight='balanced', probability=False)\n",
    "svm_clf.fit(X_train_eng_scaled, y_train)\n",
    "models['SVM (Eng. Feat.)'] = svm_clf\n",
    "\n",
    "print(\"Training Decision Tree on engineered features...\")\n",
    "dt_clf = DecisionTreeClassifier(random_state=42, class_weight='balanced', max_depth=10)\n",
    "dt_clf.fit(X_train_eng_scaled, y_train)\n",
    "models['Decision Tree (Eng. Feat.)'] = dt_clf\n",
    "\n",
    "print(\"Training KNN on engineered features...\")\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=7, n_jobs=-1) # Consider tuning n_neighbors with GridSearchCV\n",
    "knn_clf.fit(X_train_eng_scaled, y_train)\n",
    "models['KNN (Eng. Feat.)'] = knn_clf\n",
    "\n",
    "print(\"Training AdaBoost on engineered features...\")\n",
    "ada_clf = AdaBoostClassifier(n_estimators=100, random_state=42, learning_rate=0.1)\n",
    "ada_clf.fit(X_train_eng_scaled, y_train)\n",
    "models['AdaBoost (Eng. Feat.)'] = ada_clf\n",
    "\n",
    "print(\"Training XGBoost on engineered features...\")\n",
    "xgb_clf = XGBClassifier(random_state=42, n_estimators=150, use_label_encoder=False, eval_metric='mlogloss',\n",
    "                        learning_rate=0.1, max_depth=7, # Example parameters, tune these\n",
    "                        scale_pos_weight=None) # For imbalanced: compute weights or use 'balanced' logic\n",
    "# For multiclass with XGBoost, it handles labels 0..num_class-1 internally. y_train is already encoded.\n",
    "# If classes are imbalanced, consider `scale_pos_weight` (for binary) or compute sample weights.\n",
    "# For now, relying on the inherent robustness of XGBoost.\n",
    "xgb_clf.fit(X_train_eng_scaled, y_train)\n",
    "models['XGBoost (Eng. Feat.)'] = xgb_clf\n",
    "\n",
    "# --- Models using Scaled Time Series Data ---\n",
    "print(\"Training TimeSeries Forest on scaled TS data...\")\n",
    "ts_forest = TimeSeriesForestClassifier(n_estimators=150, random_state=42, n_jobs=-1) # Consider tuning\n",
    "ts_forest.fit(X_train_nested, y_train)\n",
    "models['TimeSeries Forest (Raw TS)'] = ts_forest\n",
    "\n",
    "# --- Evaluate on validation (before CNN) ---\n",
    "print(\"\\nValidation Accuracies (Traditional Models & TSF):\")\n",
    "for name, model in models.items():\n",
    "    if name == 'TimeSeries Forest (Raw TS)':\n",
    "        val_preds = model.predict(X_val_nested)\n",
    "    else: # Models trained on engineered features\n",
    "        val_preds = model.predict(X_val_eng_scaled)\n",
    "    acc = accuracy_score(y_val, val_preds)\n",
    "    val_accuracies[name] = acc\n",
    "    print(f\"{name}: {acc:.4f}\")\n",
    "    # print(classification_report(y_val, val_preds, target_names=[str(c) for c in le.classes_]))\n",
    "\n",
    "\n",
    "# --- 1D-CNN Model (PyTorch) ---\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"\\nUsing GPU for PyTorch\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"\\nUsing CPU for PyTorch\")\n",
    "\n",
    "X_train_cnn = torch.tensor(X_train_ts_scaled.reshape(-1, 1, X_train_ts_scaled.shape[1]), dtype=torch.float32)\n",
    "y_train_cnn = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val_cnn = torch.tensor(X_val_ts_scaled.reshape(-1, 1, X_val_ts_scaled.shape[1]), dtype=torch.float32)\n",
    "y_val_cnn = torch.tensor(y_val, dtype=torch.long)\n",
    "X_test_cnn_tensor = torch.tensor(X_test_ts_scaled.reshape(-1, 1, X_test_ts_scaled.shape[1]), dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_cnn, y_train_cnn)\n",
    "val_dataset = TensorDataset(X_val_cnn, y_val_cnn)\n",
    "test_dataset_cnn = TensorDataset(X_test_cnn_tensor) # Note: This only has X, no y\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader_cnn = DataLoader(test_dataset_cnn, batch_size=64, shuffle=False)\n",
    "\n",
    "class CNN1D(nn.Module):\n",
    "    def __init__(self, num_features_in, num_classes_out): # Corrected: def __init__\n",
    "        super(CNN1D, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=32, kernel_size=7, padding=3)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=7, padding=3)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2)\n",
    "\n",
    "        self.conv3 = nn.Conv1d(64, 128, kernel_size=7, padding=3)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        # Dynamic flattening size calculation\n",
    "        with torch.no_grad():\n",
    "            dummy_input_channel = 1 # Matches in_channels of self.conv1\n",
    "            dummy = torch.randn(1, dummy_input_channel, num_features_in)\n",
    "            # Pass through convolutional and pooling layers\n",
    "            dummy = self.pool1(self.relu1(self.bn1(self.conv1(dummy))))\n",
    "            dummy = self.pool2(self.relu2(self.bn2(self.conv2(dummy))))\n",
    "            dummy = self.pool3(self.relu3(self.bn3(self.conv3(dummy))))\n",
    "            flattened_size = dummy.shape[1] * dummy.shape[2]\n",
    "            # print(f\"Calculated flattened size for CNN: {flattened_size}\")\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(flattened_size, 256)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256, num_classes_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu1(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(self.relu2(self.bn2(self.conv2(x))))\n",
    "        x = self.pool3(self.relu3(self.bn3(self.conv3(x))))\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu4(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "num_input_features_cnn = X_train_ts_scaled.shape[1]\n",
    "cnn_model = CNN1D(num_features_in=num_input_features_cnn, num_classes_out=num_classes).to(device)\n",
    "try:\n",
    "    print(summary(cnn_model, input_size=(1, num_input_features_cnn)))\n",
    "except Exception as e:\n",
    "    print(f\"Error generating model summary (likely due to input shape issues if not on GPU): {e}\")\n",
    "    print(f\"Make sure model is on the same device as dummy input for summary if not using CPU.\")\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # Add label_smoothing=0.1 for regularization if desired\n",
    "optimizer = optim.Adam(cnn_model.parameters(), lr=0.0005, weight_decay=1e-4) # Try AdamW too\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)\n",
    "\n",
    "num_epochs_cnn = 60 # Or more, with early stopping\n",
    "best_val_acc_cnn = 0.0\n",
    "patience_cnn = 15 # Original patience\n",
    "patience_counter_cnn = 0\n",
    " \n",
    "print(\"\\nTraining 1D-CNN...\")\n",
    "for epoch in range(num_epochs_cnn):\n",
    "    cnn_model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = cnn_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "    cnn_model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = cnn_model(inputs)\n",
    "            loss_v = criterion(outputs, labels) # Use different var name for val loss item\n",
    "            val_loss += loss_v.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_val_loss = val_loss / len(val_loader.dataset)\n",
    "    epoch_val_acc = correct / total\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs_cnn}, Train Loss: {epoch_loss:.4f}, Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.4f}\")\n",
    "\n",
    "    scheduler.step(epoch_val_loss)\n",
    "\n",
    "    if epoch_val_acc > best_val_acc_cnn:\n",
    "        best_val_acc_cnn = epoch_val_acc\n",
    "        torch.save(cnn_model.state_dict(), 'best_cnn_1d_model.pth')\n",
    "        print(f\"New best CNN val acc: {best_val_acc_cnn:.4f}. Model saved.\")\n",
    "        patience_counter_cnn = 0\n",
    "    else:\n",
    "        patience_counter_cnn += 1\n",
    "    \n",
    "    if patience_counter_cnn >= patience_cnn:\n",
    "        print(f\"CNN Early stopping at epoch {epoch+1}.\")\n",
    "        break\n",
    "\n",
    "# Load best model for final evaluation and prediction\n",
    "if best_val_acc_cnn > 0: # Check if any model was saved\n",
    "    print(f\"Loading best CNN model with val acc: {best_val_acc_cnn:.4f}\")\n",
    "    cnn_model.load_state_dict(torch.load('best_cnn_1d_model.pth'))\n",
    "else: # Should not happen if training ran at least one epoch\n",
    "    print(\"No best CNN model saved, using last state (potential issue).\")\n",
    "\n",
    "\n",
    "models['1D-CNN (Raw TS)'] = cnn_model # Store the PyTorch model itself\n",
    "val_accuracies['1D-CNN (Raw TS)'] = best_val_acc_cnn # Store its best validation accuracy\n",
    "print(f\"Best 1D-CNN Validation Accuracy after training: {best_val_acc_cnn:.4f}\")\n",
    "\n",
    "\n",
    "#==== Final Predictions on Test Set ====\n",
    "print(\"\\nPredicting on test set and saving results...\")\n",
    "all_predictions_df = pd.DataFrame() # To store all predictions if needed later\n",
    "\n",
    "for name, model_object in models.items():\n",
    "    filename_prefix = name.lower().replace(' (eng. feat.)', '_eng_feat') \\\n",
    "                                .replace(' (raw ts)', '_raw_ts') \\\n",
    "                                .replace(' ', '_').replace('(', '').replace(')', '')\n",
    "    preds_original_labels = None # Initialize\n",
    "\n",
    "    if name == 'TimeSeries Forest (Raw TS)':\n",
    "        test_preds = model_object.predict(X_test_nested)\n",
    "        preds_original_labels = le.inverse_transform(test_preds)\n",
    "    elif name == '1D-CNN (Raw TS)':\n",
    "        model_object.eval() # Ensure model is in evaluation mode\n",
    "        cnn_test_preds_list = []\n",
    "        with torch.no_grad():\n",
    "            for inputs_batch in test_loader_cnn:\n",
    "                inputs_tensor = inputs_batch[0].to(device)\n",
    "                outputs = model_object(inputs_tensor)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                cnn_test_preds_list.extend(predicted.cpu().numpy())\n",
    "        test_preds = np.array(cnn_test_preds_list)\n",
    "        preds_original_labels = le.inverse_transform(test_preds)\n",
    "    else: # Models trained on engineered features\n",
    "        test_preds = model_object.predict(X_test_eng_scaled)\n",
    "        preds_original_labels = le.inverse_transform(test_preds)\n",
    "\n",
    "    if preds_original_labels is not None:\n",
    "        # Store predictions in a common structure if you want to analyze them together later\n",
    "        all_predictions_df[name] = preds_original_labels \n",
    "        \n",
    "        # Save individual prediction files\n",
    "        pd.DataFrame(preds_original_labels).to_csv(f\"{filename_prefix}_predictions.csv\", index=False, header=False)\n",
    "        print(f\"Saved: {filename_prefix}_predictions.csv\")\n",
    "    else:\n",
    "        print(f\"Could not generate predictions for {name}\")\n",
    "\n",
    "print(\"\\n--- Final Validation Accuracies Summary ---\")\n",
    "# Sort by accuracy\n",
    "sorted_val_accuracies = sorted(val_accuracies.items(), key=lambda item: item[1], reverse=True)\n",
    "for model_name, acc in sorted_val_accuracies:\n",
    "    print(f\"{model_name}: {acc:.4f}\")\n",
    "\n",
    "best_model_name = sorted_val_accuracies[0][0] if sorted_val_accuracies else \"N/A\"\n",
    "best_model_accuracy = sorted_val_accuracies[0][1] if sorted_val_accuracies else 0.0\n",
    "print(f\"\\nBest performing model on validation set: {best_model_name} with accuracy: {best_model_accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nAll operations complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90140d4",
   "metadata": {},
   "source": [
    "# Data # \n",
    "#### Training Data: ####\n",
    "- Sleep Train 5000.csv (5000 samples, 179 columns: 1 label + 178 signal time points).\n",
    "#### Test Data: ####\n",
    "- Sleep Test 1000.csv (1000 samples, 178 signal time points).\n",
    "\n",
    "# Methodology #\n",
    "## 1. Data Preprocessing ##\n",
    "- Labels were string-encoded and then numerically encoded using LabelEncoder.\n",
    "- Data was split into training (80%) and validation (20%) sets, stratified by class.\n",
    "#### Two main data preparation paths were followed: ####\n",
    "##### Engineered Features: #####\n",
    "- Raw signals were processed to extract statistical, FFT, and Wavelet features.\n",
    "- ARIMA and Prophet features were available but disabled for this run due to computational cost.\n",
    "These engineered features were then scaled using StandardScaler.\n",
    "##### Raw Time Series: #####\n",
    "- Raw signals were directly scaled using a separate StandardScaler for use with time-series specific models (TimeSeries Forest, 1D-CNN).\n",
    "\n",
    "## 2. Feature Engineering ##\n",
    "- **Statistical Features (18 features):**\n",
    "Mean, std, var, min, max, range, median, skew, kurtosis, zero-crossings, peaks, mean/std of differences, energy, power, quartiles, IQR.\n",
    "- **FFT Features (50 + 5 + 4 = 59 features):**\n",
    "50 dominant frequency coefficients, 5 power band ratios, and mean/std/max/peak index of the FFT spectrum.\n",
    "- **Wavelet Features ( (4+1)*3 = 15 features):**\n",
    "Mean, std, and energy of approximation and detail coefficients from a 4-level 'db4' wavelet decomposition.\n",
    "- **Total Engineered Features (without ARIMA/Prophet):**\n",
    "18 + 59 + 15 = 92 features.\n",
    "\n",
    "## 3. Model Training & Evaluation ##\n",
    "A diverse set of models was trained and evaluated:\n",
    "### Models on Engineered Features: ###\n",
    "- Logistic Regression (with class balancing)\n",
    "- Random Forest (with GridSearchCV for hyperparameter tuning: n_estimators, max_depth, min_samples_split/leaf, class_weight)\n",
    "- Support Vector Machine (SVC with RBF kernel, class balancing)\n",
    "- Decision Tree (with class balancing, max_depth=10)\n",
    "- K-Nearest Neighbors (KNN, n_neighbors=7)\n",
    "- AdaBoost Classifier\n",
    "- XGBoost Classifier (initial run with default-ish parameters)\n",
    "\n",
    "### Models on Scaled Raw Time Series: ###\n",
    "- TimeSeries Forest Classifier (n_estimators=150)\n",
    "- 1D Convolutional Neural Network (CNN) via PyTorch:\n",
    "- **Architecture:** 3 Conv1D layers (kernels=7, padding=3, MaxPool1D after each), BatchNorm, ReLU, followed by Flatten, Dropout (0.5), and 2 Dense layers.\n",
    "- **Optimizer:** Adam (lr=0.0005, weight_decay=1e-4).\n",
    "- **Loss:** CrossEntropyLoss.\n",
    "- **Training:** 60 epochs with early stopping (patience=15) and ReduceLROnPlateau scheduler based on validation loss.\n",
    "\n",
    "## 4. Results (Based on Validation Set Performance) ##\n",
    "- 1D-CNN (Raw TS): 0.7190\n",
    "- XGBoost (Eng. Feat.): 0.7010\n",
    "- Random Forest (Eng. Feat.): 0.6880\n",
    "- SVM (Eng. Feat.): 0.6650\n",
    "- Logistic Regression (Eng. Feat.): 0.6370\n",
    "- KNN (Eng. Feat.): 0.6300\n",
    "- Decision Tree (Eng. Feat.): 0.5730\n",
    "- TimeSeries Forest (Raw TS): 0.5660\n",
    "- AdaBoost (Eng. Feat.): 0.5450\n",
    "\n",
    "- The 1D-CNN trained on raw scaled time series achieved the highest validation accuracy (0.7200).\n",
    "- Among models using engineered features, XGBoost (0.7010) and Random Forest (0.6880) performed best.\n",
    "- The initial TimeSeries Forest classifier performed less competitively on this dataset compared to the CNN and top engineered feature models.\n",
    "\n",
    "## 5. Conclusions & Next Steps ##\n",
    "The initial pipeline demonstrates the feasibility of classifying sleep stages, with the 1D-CNN showing the most promise. The engineered features also provide a solid foundation for traditional ensemble models.\n",
    "Recommendations for Improvement (to reach ~0.8 accuracy target):\n",
    "Systematic Hyperparameter Tuning:\n",
    "- Expand GridSearchCV for XGBoost and Random Forest.\n",
    "- Utilize Optuna (or similar) for more comprehensive CNN hyperparameter search (architecture, optimizer, regularization).\n",
    "- Advanced Ensemble Methods: Explore stacking or weighted voting of the best performing models (e.g., CNN, XGBoost, RF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da147ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training XGBoost on engineered features (with EXPANDED GridSearchCV)...\n",
      "Fitting 3 folds for each of 432 candidates, totalling 1296 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Semhane\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\training.py:183: UserWarning: [00:29:04] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best XGBoost params: {'colsample_bytree': 0.8, 'gamma': 0, 'learning_rate': 0.05, 'max_depth': 7, 'min_child_weight': 3, 'n_estimators': 300, 'subsample': 0.9}\n",
      "Best XGBoost validation accuracy (from CV): 0.7069\n",
      "\n",
      "Using CPU for PyTorch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 00:29:23,313] A new study created in memory with name: no-name-df0eec55-3ad2-4058-bc4a-402e67b538a0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Optuna study for 1D-CNN hyperparameter tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 00:32:20,155] Trial 0 finished with value: 0.721 and parameters: {'lr': 0.0006331380541711627, 'weight_decay': 0.00015331280482733912, 'dropout_rate': 0.3129501379937218, 'batch_size': 128, 'n_filters_1': 64, 'n_filters_2': 128, 'n_filters_3': 256, 'fc_units': 256, 'label_smoothing': 0.033456628820654304}. Best is trial 0 with value: 0.721.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna Trial 0: Early stopping at epoch 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 00:33:23,092] Trial 1 finished with value: 0.731 and parameters: {'lr': 0.0017267273678882062, 'weight_decay': 4.053082576837675e-05, 'dropout_rate': 0.3769760277781474, 'batch_size': 64, 'n_filters_1': 64, 'n_filters_2': 64, 'n_filters_3': 128, 'fc_units': 512, 'label_smoothing': 0.03896307396854329}. Best is trial 1 with value: 0.731.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna Trial 1: Early stopping at epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 00:34:29,620] Trial 2 finished with value: 0.748 and parameters: {'lr': 0.001625383638022807, 'weight_decay': 0.002978120991290357, 'dropout_rate': 0.6701259674364999, 'batch_size': 64, 'n_filters_1': 32, 'n_filters_2': 128, 'n_filters_3': 128, 'fc_units': 256, 'label_smoothing': 0.1627328882060397}. Best is trial 2 with value: 0.748.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna Trial 2: Early stopping at epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 00:36:12,238] Trial 3 finished with value: 0.712 and parameters: {'lr': 0.004185367667937767, 'weight_decay': 1.5102428966400838e-05, 'dropout_rate': 0.2825824798329591, 'batch_size': 128, 'n_filters_1': 64, 'n_filters_2': 64, 'n_filters_3': 256, 'fc_units': 128, 'label_smoothing': 0.11862645575937283}. Best is trial 2 with value: 0.748.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna Trial 3: Early stopping at epoch 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 00:38:24,299] Trial 4 finished with value: 0.722 and parameters: {'lr': 0.004071766942625743, 'weight_decay': 7.992229739370787e-05, 'dropout_rate': 0.5910000639140549, 'batch_size': 128, 'n_filters_1': 32, 'n_filters_2': 64, 'n_filters_3': 256, 'fc_units': 512, 'label_smoothing': 0.11676846914005745}. Best is trial 2 with value: 0.748.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna Trial 4: Early stopping at epoch 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 00:39:31,156] Trial 5 pruned. \n",
      "[I 2025-05-17 00:39:48,828] Trial 6 pruned. \n",
      "[I 2025-05-17 00:40:08,234] Trial 7 pruned. \n",
      "[I 2025-05-17 00:42:24,800] Trial 8 finished with value: 0.741 and parameters: {'lr': 0.0005603664008962393, 'weight_decay': 0.00021636638449636515, 'dropout_rate': 0.6984849825740679, 'batch_size': 64, 'n_filters_1': 64, 'n_filters_2': 64, 'n_filters_3': 256, 'fc_units': 256, 'label_smoothing': 0.07309536279041605}. Best is trial 2 with value: 0.748.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna Trial 8: Early stopping at epoch 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 00:42:58,978] Trial 9 pruned. \n",
      "[I 2025-05-17 00:43:02,416] Trial 10 pruned. \n",
      "[I 2025-05-17 00:45:10,268] Trial 11 finished with value: 0.742 and parameters: {'lr': 0.0005208835635010251, 'weight_decay': 0.0006252726481687836, 'dropout_rate': 0.6978604148233597, 'batch_size': 64, 'n_filters_1': 64, 'n_filters_2': 128, 'n_filters_3': 128, 'fc_units': 256, 'label_smoothing': 0.07303772610745499}. Best is trial 2 with value: 0.748.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna Trial 11: Early stopping at epoch 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 00:45:20,305] Trial 12 pruned. \n",
      "[I 2025-05-17 00:45:24,287] Trial 13 pruned. \n",
      "[I 2025-05-17 00:45:36,481] Trial 14 pruned. \n",
      "[I 2025-05-17 00:45:46,470] Trial 15 pruned. \n",
      "[I 2025-05-17 00:47:21,657] Trial 16 finished with value: 0.743 and parameters: {'lr': 0.0007362469070673008, 'weight_decay': 0.00047431015371211436, 'dropout_rate': 0.6372490412417058, 'batch_size': 64, 'n_filters_1': 64, 'n_filters_2': 128, 'n_filters_3': 128, 'fc_units': 128, 'label_smoothing': 0.06337841467203796}. Best is trial 2 with value: 0.748.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna Trial 16: Early stopping at epoch 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 00:47:37,481] Trial 17 pruned. \n",
      "[I 2025-05-17 00:48:25,716] Trial 18 pruned. \n",
      "[I 2025-05-17 00:48:29,569] Trial 19 pruned. \n",
      "[I 2025-05-17 00:48:51,844] Trial 20 pruned. \n",
      "[I 2025-05-17 00:50:55,838] Trial 21 finished with value: 0.744 and parameters: {'lr': 0.0004776220299944409, 'weight_decay': 0.0006367977605138611, 'dropout_rate': 0.6828917015427027, 'batch_size': 64, 'n_filters_1': 64, 'n_filters_2': 128, 'n_filters_3': 128, 'fc_units': 256, 'label_smoothing': 0.0649487446580778}. Best is trial 2 with value: 0.748.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna Trial 21: Early stopping at epoch 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 00:52:19,867] Trial 22 finished with value: 0.741 and parameters: {'lr': 0.0009609683753814958, 'weight_decay': 0.0019494703533282606, 'dropout_rate': 0.6520015645220743, 'batch_size': 64, 'n_filters_1': 64, 'n_filters_2': 128, 'n_filters_3': 128, 'fc_units': 256, 'label_smoothing': 0.04874191956660638}. Best is trial 2 with value: 0.748.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna Trial 22: Early stopping at epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 00:53:35,437] Trial 23 finished with value: 0.73 and parameters: {'lr': 0.00037292174394559845, 'weight_decay': 0.0003079671550362641, 'dropout_rate': 0.6003673374746339, 'batch_size': 64, 'n_filters_1': 64, 'n_filters_2': 128, 'n_filters_3': 128, 'fc_units': 256, 'label_smoothing': 0.05462582690196247}. Best is trial 2 with value: 0.748.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna Trial 23: Early stopping at epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 00:53:50,729] Trial 24 pruned. \n",
      "[I 2025-05-17 00:54:35,997] Trial 25 pruned. \n",
      "[I 2025-05-17 00:54:42,984] Trial 26 pruned. \n",
      "[I 2025-05-17 00:54:47,471] Trial 27 pruned. \n",
      "[I 2025-05-17 00:54:50,847] Trial 28 pruned. \n",
      "[I 2025-05-17 00:57:07,900] Trial 29 finished with value: 0.73 and parameters: {'lr': 0.0007708825774272593, 'weight_decay': 8.479111953364705e-05, 'dropout_rate': 0.5575818082179422, 'batch_size': 32, 'n_filters_1': 64, 'n_filters_2': 128, 'n_filters_3': 128, 'fc_units': 128, 'label_smoothing': 0.08099484542704519}. Best is trial 2 with value: 0.748.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna Trial 29: Early stopping at epoch 30\n",
      "Optuna study finished.\n",
      "Best CNN trial:\n",
      "  Value (Val Acc): 0.7480\n",
      "  Params: \n",
      "    lr: 0.001625383638022807\n",
      "    weight_decay: 0.002978120991290357\n",
      "    dropout_rate: 0.6701259674364999\n",
      "    batch_size: 64\n",
      "    n_filters_1: 32\n",
      "    n_filters_2: 128\n",
      "    n_filters_3: 128\n",
      "    fc_units: 256\n",
      "    label_smoothing: 0.1627328882060397\n",
      "\n",
      "Training final 1D-CNN with best Optuna hyperparameters...\n",
      "Final CNN Train - Epoch 1/150, Train Loss: 1.3502, Val Loss: 1.1871, Val Acc: 0.6120\n",
      "New best tuned CNN val acc: 0.6120. Model saved.\n",
      "Final CNN Train - Epoch 2/150, Train Loss: 1.1865, Val Loss: 1.1843, Val Acc: 0.5980\n",
      "Final CNN Train - Epoch 3/150, Train Loss: 1.1706, Val Loss: 1.1370, Val Acc: 0.6170\n",
      "New best tuned CNN val acc: 0.6170. Model saved.\n",
      "Final CNN Train - Epoch 4/150, Train Loss: 1.0931, Val Loss: 1.0960, Val Acc: 0.6700\n",
      "New best tuned CNN val acc: 0.6700. Model saved.\n",
      "Final CNN Train - Epoch 5/150, Train Loss: 1.0768, Val Loss: 1.1585, Val Acc: 0.6160\n",
      "Final CNN Train - Epoch 6/150, Train Loss: 1.0417, Val Loss: 1.0612, Val Acc: 0.6920\n",
      "New best tuned CNN val acc: 0.6920. Model saved.\n",
      "Final CNN Train - Epoch 7/150, Train Loss: 1.0320, Val Loss: 1.1316, Val Acc: 0.6580\n",
      "Final CNN Train - Epoch 8/150, Train Loss: 1.0074, Val Loss: 1.0560, Val Acc: 0.6990\n",
      "New best tuned CNN val acc: 0.6990. Model saved.\n",
      "Final CNN Train - Epoch 9/150, Train Loss: 0.9976, Val Loss: 1.0174, Val Acc: 0.7270\n",
      "New best tuned CNN val acc: 0.7270. Model saved.\n",
      "Final CNN Train - Epoch 10/150, Train Loss: 0.9970, Val Loss: 1.1004, Val Acc: 0.6860\n",
      "Final CNN Train - Epoch 11/150, Train Loss: 0.9884, Val Loss: 1.0504, Val Acc: 0.7090\n",
      "Final CNN Train - Epoch 12/150, Train Loss: 0.9720, Val Loss: 1.0524, Val Acc: 0.7060\n",
      "Final CNN Train - Epoch 13/150, Train Loss: 0.9492, Val Loss: 1.2113, Val Acc: 0.6080\n",
      "Final CNN Train - Epoch 14/150, Train Loss: 0.9480, Val Loss: 1.0247, Val Acc: 0.7370\n",
      "New best tuned CNN val acc: 0.7370. Model saved.\n",
      "Final CNN Train - Epoch 15/150, Train Loss: 0.9243, Val Loss: 1.0524, Val Acc: 0.7230\n",
      "Final CNN Train - Epoch 16/150, Train Loss: 0.9242, Val Loss: 1.0990, Val Acc: 0.6800\n",
      "Final CNN Train - Epoch 17/150, Train Loss: 0.9088, Val Loss: 1.1766, Val Acc: 0.6430\n",
      "Final CNN Train - Epoch 18/150, Train Loss: 0.9129, Val Loss: 1.0573, Val Acc: 0.7140\n",
      "Final CNN Train - Epoch 19/150, Train Loss: 0.8778, Val Loss: 1.0568, Val Acc: 0.7060\n",
      "Final CNN Train - Epoch 20/150, Train Loss: 0.8786, Val Loss: 1.0783, Val Acc: 0.7010\n",
      "Final CNN Train - Epoch 21/150, Train Loss: 0.8473, Val Loss: 1.0121, Val Acc: 0.7410\n",
      "New best tuned CNN val acc: 0.7410. Model saved.\n",
      "Final CNN Train - Epoch 22/150, Train Loss: 0.8277, Val Loss: 1.0220, Val Acc: 0.7470\n",
      "New best tuned CNN val acc: 0.7470. Model saved.\n",
      "Final CNN Train - Epoch 23/150, Train Loss: 0.8157, Val Loss: 1.0028, Val Acc: 0.7410\n",
      "Final CNN Train - Epoch 24/150, Train Loss: 0.8148, Val Loss: 1.0365, Val Acc: 0.7210\n",
      "Final CNN Train - Epoch 25/150, Train Loss: 0.8092, Val Loss: 1.0757, Val Acc: 0.6920\n",
      "Final CNN Train - Epoch 26/150, Train Loss: 0.7952, Val Loss: 1.0405, Val Acc: 0.7310\n",
      "Final CNN Train - Epoch 27/150, Train Loss: 0.7815, Val Loss: 1.0160, Val Acc: 0.7450\n",
      "Final CNN Train - Epoch 28/150, Train Loss: 0.7708, Val Loss: 1.0552, Val Acc: 0.7220\n",
      "Final CNN Train - Epoch 29/150, Train Loss: 0.7847, Val Loss: 1.0968, Val Acc: 0.7180\n",
      "Final CNN Train - Epoch 30/150, Train Loss: 0.7743, Val Loss: 1.0285, Val Acc: 0.7360\n",
      "Final CNN Train - Epoch 31/150, Train Loss: 0.7628, Val Loss: 1.0124, Val Acc: 0.7420\n",
      "Final CNN Train - Epoch 32/150, Train Loss: 0.7552, Val Loss: 1.0399, Val Acc: 0.7270\n",
      "Final CNN Train - Epoch 33/150, Train Loss: 0.7441, Val Loss: 1.0815, Val Acc: 0.7230\n",
      "Final CNN Train - Epoch 34/150, Train Loss: 0.7426, Val Loss: 1.1342, Val Acc: 0.6890\n",
      "Final CNN Train - Epoch 35/150, Train Loss: 0.7281, Val Loss: 1.0333, Val Acc: 0.7270\n",
      "Final CNN Train - Epoch 36/150, Train Loss: 0.7250, Val Loss: 1.0581, Val Acc: 0.7240\n",
      "Final CNN Train - Epoch 37/150, Train Loss: 0.7108, Val Loss: 1.0347, Val Acc: 0.7380\n",
      "Final CNN Train - Epoch 38/150, Train Loss: 0.7159, Val Loss: 1.0531, Val Acc: 0.7340\n",
      "Final CNN Train - Epoch 39/150, Train Loss: 0.7038, Val Loss: 1.0141, Val Acc: 0.7500\n",
      "New best tuned CNN val acc: 0.7500. Model saved.\n",
      "Final CNN Train - Epoch 40/150, Train Loss: 0.7038, Val Loss: 1.0387, Val Acc: 0.7180\n",
      "Final CNN Train - Epoch 41/150, Train Loss: 0.7010, Val Loss: 1.0763, Val Acc: 0.7320\n",
      "Final CNN Train - Epoch 42/150, Train Loss: 0.7018, Val Loss: 1.0372, Val Acc: 0.7330\n",
      "Final CNN Train - Epoch 43/150, Train Loss: 0.6975, Val Loss: 1.0450, Val Acc: 0.7370\n",
      "Final CNN Train - Epoch 44/150, Train Loss: 0.6898, Val Loss: 1.0262, Val Acc: 0.7420\n",
      "Final CNN Train - Epoch 45/150, Train Loss: 0.6899, Val Loss: 1.0333, Val Acc: 0.7430\n",
      "Final CNN Train - Epoch 46/150, Train Loss: 0.6824, Val Loss: 1.0323, Val Acc: 0.7400\n",
      "Final CNN Train - Epoch 47/150, Train Loss: 0.6777, Val Loss: 1.0272, Val Acc: 0.7360\n",
      "Final CNN Train - Epoch 48/150, Train Loss: 0.6788, Val Loss: 1.0539, Val Acc: 0.7260\n",
      "Final CNN Train - Epoch 49/150, Train Loss: 0.6757, Val Loss: 1.0280, Val Acc: 0.7390\n",
      "Final CNN Train - Epoch 50/150, Train Loss: 0.6821, Val Loss: 1.0322, Val Acc: 0.7340\n",
      "Final CNN Train - Epoch 51/150, Train Loss: 0.6744, Val Loss: 1.0688, Val Acc: 0.7290\n",
      "Final CNN Train - Epoch 52/150, Train Loss: 0.6754, Val Loss: 1.0300, Val Acc: 0.7390\n",
      "Final CNN Train - Epoch 53/150, Train Loss: 0.6709, Val Loss: 1.0417, Val Acc: 0.7420\n",
      "Final CNN Train - Epoch 54/150, Train Loss: 0.6733, Val Loss: 1.0589, Val Acc: 0.7340\n",
      "Final CNN Train - Epoch 55/150, Train Loss: 0.6684, Val Loss: 1.0478, Val Acc: 0.7310\n",
      "Final CNN Train - Epoch 56/150, Train Loss: 0.6735, Val Loss: 1.0466, Val Acc: 0.7330\n",
      "Final CNN Train - Epoch 57/150, Train Loss: 0.6675, Val Loss: 1.0297, Val Acc: 0.7400\n",
      "Final CNN Train - Epoch 58/150, Train Loss: 0.6677, Val Loss: 1.0309, Val Acc: 0.7490\n",
      "Final CNN Train - Epoch 59/150, Train Loss: 0.6661, Val Loss: 1.0271, Val Acc: 0.7360\n",
      "Final CNN Train - Epoch 60/150, Train Loss: 0.6666, Val Loss: 1.0368, Val Acc: 0.7340\n",
      "Final CNN Train - Epoch 61/150, Train Loss: 0.6616, Val Loss: 1.0371, Val Acc: 0.7300\n",
      "Final CNN Train - Epoch 62/150, Train Loss: 0.6592, Val Loss: 1.0397, Val Acc: 0.7360\n",
      "Final CNN Train - Epoch 63/150, Train Loss: 0.6587, Val Loss: 1.0369, Val Acc: 0.7360\n",
      "Final CNN Train - Epoch 64/150, Train Loss: 0.6587, Val Loss: 1.0338, Val Acc: 0.7360\n",
      "Final CNN Early stopping at epoch 64.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 288\u001b[0m\n\u001b[0;32m    285\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# Load the best tuned CNN model for evaluation and prediction\u001b[39;00m\n\u001b[1;32m--> 288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_tuned_cnn_1d_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    289\u001b[0m     final_cnn_model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_tuned_cnn_1d_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m    290\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded best tuned CNN model with val acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_val_acc_cnn_final\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold # Ensure this is imported\n",
    "\n",
    "# ==== XGBoost Training with GridSearchCV (Refined) ====\n",
    "print(\"\\nTraining XGBoost on engineered features (with EXPANDED GridSearchCV)...\")\n",
    "\n",
    "cv_stratified = StratifiedKFold(n_splits=3, shuffle=True, random_state=42) # 3 splits for speed, 5 for better estimate\n",
    "\n",
    "\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [200, 300, 400],          # Number of trees\n",
    "    'learning_rate': [0.03, 0.05, 0.1],       # Step size shrinkage\n",
    "    'max_depth': [5, 7, 9],                   # Max depth of a tree\n",
    "    'subsample': [0.8, 0.9],                  # Subsample ratio of the training instance\n",
    "    'colsample_bytree': [0.8, 0.9],           # Subsample ratio of columns when constructing each tree\n",
    "    'gamma': [0, 0.1],                        # Minimum loss reduction required to make a further partition\n",
    "    'min_child_weight': [1, 3]                # Minimum sum of instance weight (hessian) needed in a child\n",
    "}\n",
    "\n",
    "xgb_grid_search = GridSearchCV(\n",
    "    XGBClassifier(random_state=42, eval_metric='mlogloss', use_label_encoder=False), # use_label_encoder=False for newer XGBoost\n",
    "    param_grid_xgb,\n",
    "    cv=cv_stratified,\n",
    "    scoring='accuracy',\n",
    "    verbose=2, # Set to 1 or 0 for less output\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "\n",
    "xgb_grid_search.fit(X_train_eng_scaled, y_train) # Make sure these are your scaled engineered features\n",
    "\n",
    "print(f\"Best XGBoost params: {xgb_grid_search.best_params_}\")\n",
    "print(f\"Best XGBoost validation accuracy (from CV): {xgb_grid_search.best_score_:.4f}\")\n",
    "\n",
    "xgb_clf = xgb_grid_search.best_estimator_ # This is your tuned XGBoost\n",
    "models['XGBoost (Eng. Feat.)'] = xgb_clf\n",
    "# ==== 1D-CNN Model with Optuna Hyperparameter Tuning ====\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"\\nUsing GPU for PyTorch\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"\\nUsing CPU for PyTorch\")\n",
    "\n",
    "# Prepare data tensors (ensure these are defined: X_train_ts_scaled, y_train, X_val_ts_scaled, y_val)\n",
    "X_train_cnn_opt = torch.tensor(X_train_ts_scaled.reshape(-1, 1, X_train_ts_scaled.shape[1]), dtype=torch.float32)\n",
    "y_train_cnn_opt = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val_cnn_opt = torch.tensor(X_val_ts_scaled.reshape(-1, 1, X_val_ts_scaled.shape[1]), dtype=torch.float32)\n",
    "y_val_cnn_opt = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "# Global variable for input features, will be set before Optuna study\n",
    "num_input_features_cnn_global = X_train_ts_scaled.shape[1]\n",
    "num_classes_global = num_classes # num_classes should be defined from LabelEncoder\n",
    "\n",
    "# Define the CNN1D class (as you have it, ensure __init__ is correct)\n",
    "class CNN1D(nn.Module):\n",
    "    def __init__(self, num_features_in, num_classes_out, n_filters_1, n_filters_2, n_filters_3, fc_units, dropout_rate):\n",
    "        super(CNN1D, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=n_filters_1, kernel_size=7, padding=3)\n",
    "        self.bn1 = nn.BatchNorm1d(n_filters_1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(n_filters_1, n_filters_2, kernel_size=7, padding=3)\n",
    "        self.bn2 = nn.BatchNorm1d(n_filters_2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2)\n",
    "\n",
    "        self.conv3 = nn.Conv1d(n_filters_2, n_filters_3, kernel_size=7, padding=3)\n",
    "        self.bn3 = nn.BatchNorm1d(n_filters_3)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            dummy_input_channel = 1\n",
    "            dummy = torch.randn(1, dummy_input_channel, num_features_in)\n",
    "            dummy = self.pool1(self.relu1(self.bn1(self.conv1(dummy))))\n",
    "            dummy = self.pool2(self.relu2(self.bn2(self.conv2(dummy))))\n",
    "            dummy = self.pool3(self.relu3(self.bn3(self.conv3(dummy))))\n",
    "            flattened_size = dummy.shape[1] * dummy.shape[2]\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc1 = nn.Linear(flattened_size, fc_units)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(fc_units, num_classes_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu1(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(self.relu2(self.bn2(self.conv2(x))))\n",
    "        x = self.pool3(self.relu3(self.bn3(self.conv3(x))))\n",
    "        x = self.flatten(x); x = self.dropout(x)\n",
    "        x = self.relu4(self.fc1(x)); x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Optuna Objective Function\n",
    "def objective_cnn(trial):\n",
    "    # Hyperparameters to tune\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-5, 1e-2, log=True)\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.2, 0.7)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "    \n",
    "    n_filters_1 = trial.suggest_categorical(\"n_filters_1\", [32, 64])\n",
    "    n_filters_2 = trial.suggest_categorical(\"n_filters_2\", [64, 128])\n",
    "    n_filters_3 = trial.suggest_categorical(\"n_filters_3\", [128, 256])\n",
    "    fc_units = trial.suggest_categorical(\"fc_units\", [128, 256, 512])\n",
    "    \n",
    "    label_smoothing = trial.suggest_float(\"label_smoothing\", 0.0, 0.2) # Tunable label smoothing\n",
    "\n",
    "    # Model, Optimizer, Criterion\n",
    "    model = CNN1D(\n",
    "        num_features_in=num_input_features_cnn_global,\n",
    "        num_classes_out=num_classes_global,\n",
    "        n_filters_1=n_filters_1,\n",
    "        n_filters_2=n_filters_2,\n",
    "        n_filters_3=n_filters_3,\n",
    "        fc_units=fc_units,\n",
    "        dropout_rate=dropout_rate\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    # Handle class weights if y_train is imbalanced\n",
    "    # Consider re-calculating unique_classes and y_train if they are not globally accessible or are modified\n",
    "    # For simplicity, assuming y_train (original labels before encoding for CNN) is available for weight calculation\n",
    "    # If class imbalance is a major issue, this should be done carefully\n",
    "    # class_weights_array = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "    # class_weights_tensor = torch.tensor(class_weights_array, dtype=torch.float32).to(device)\n",
    "    # criterion = nn.CrossEntropyLoss(weight=class_weights_tensor, label_smoothing=label_smoothing)\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing) # Simpler version without weights for now\n",
    "\n",
    "\n",
    "    # DataLoaders\n",
    "    train_dataset_opt = TensorDataset(X_train_cnn_opt, y_train_cnn_opt)\n",
    "    val_dataset_opt = TensorDataset(X_val_cnn_opt, y_val_cnn_opt)\n",
    "    train_loader_opt = DataLoader(train_dataset_opt, batch_size=batch_size, shuffle=True)\n",
    "    val_loader_opt = DataLoader(val_dataset_opt, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Training loop for Optuna (simplified for brevity, add your full loop with early stopping)\n",
    "    num_epochs_optuna_trial = 50 # Number of epochs PER Optuna trial (can be less than final training)\n",
    "    best_val_acc_trial = 0.0\n",
    "    patience_optuna_trial = 10 # Early stopping patience for the trial\n",
    "    patience_counter_trial = 0\n",
    "\n",
    "    for epoch in range(num_epochs_optuna_trial):\n",
    "        model.train()\n",
    "        for inputs, labels in train_loader_opt:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        val_loss_epoch = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader_opt:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss_v = criterion(outputs, labels)\n",
    "                val_loss_epoch += loss_v.item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        epoch_val_acc = correct / total\n",
    "        # epoch_val_loss_avg = val_loss_epoch / len(val_loader_opt.dataset) # Optional: for scheduler based on loss\n",
    "\n",
    "        if epoch_val_acc > best_val_acc_trial:\n",
    "            best_val_acc_trial = epoch_val_acc\n",
    "            patience_counter_trial = 0\n",
    "        else:\n",
    "            patience_counter_trial += 1\n",
    "        \n",
    "        if patience_counter_trial >= patience_optuna_trial:\n",
    "            print(f\"Optuna Trial {trial.number}: Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "        \n",
    "        # Optuna pruning: report intermediate value and check if trial should be pruned\n",
    "        trial.report(epoch_val_acc, epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "            \n",
    "    return best_val_acc_trial # Optuna tries to maximize this value\n",
    "\n",
    "# --- Run Optuna Study ---\n",
    "print(\"\\nStarting Optuna study for 1D-CNN hyperparameter tuning...\")\n",
    "# You can add a pruner for more efficient search, e.g., MedianPruner\n",
    "study_cnn = optuna.create_study(direction=\"maximize\", pruner=optuna.pruners.MedianPruner())\n",
    "study_cnn.optimize(objective_cnn, n_trials=30)  # Adjust n_trials (e.g., 20-100 depending on time)\n",
    "\n",
    "print(\"Optuna study finished.\")\n",
    "print(\"Best CNN trial:\")\n",
    "best_trial_cnn = study_cnn.best_trial\n",
    "print(f\"  Value (Val Acc): {best_trial_cnn.value:.4f}\")\n",
    "print(\"  Params: \")\n",
    "for key, value in best_trial_cnn.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "# --- Train final CNN model with best Optuna hyperparameters ---\n",
    "print(\"\\nTraining final 1D-CNN with best Optuna hyperparameters...\")\n",
    "best_cnn_params = best_trial_cnn.params\n",
    "\n",
    "final_cnn_model = CNN1D(\n",
    "    num_features_in=num_input_features_cnn_global,\n",
    "    num_classes_out=num_classes_global,\n",
    "    n_filters_1=best_cnn_params['n_filters_1'],\n",
    "    n_filters_2=best_cnn_params['n_filters_2'],\n",
    "    n_filters_3=best_cnn_params['n_filters_3'],\n",
    "    fc_units=best_cnn_params['fc_units'],\n",
    "    dropout_rate=best_cnn_params['dropout_rate']\n",
    ").to(device)\n",
    "\n",
    "final_optimizer = optim.AdamW(\n",
    "    final_cnn_model.parameters(),\n",
    "    lr=best_cnn_params['lr'],\n",
    "    weight_decay=best_cnn_params['weight_decay']\n",
    ")\n",
    "# Re-calculate class weights if needed for the final model\n",
    "# final_criterion = nn.CrossEntropyLoss(weight=class_weights_tensor, label_smoothing=best_cnn_params['label_smoothing'])\n",
    "final_criterion = nn.CrossEntropyLoss(label_smoothing=best_cnn_params['label_smoothing'])\n",
    "\n",
    "\n",
    "final_batch_size = best_cnn_params['batch_size']\n",
    "train_dataset_final = TensorDataset(X_train_cnn_opt, y_train_cnn_opt) # Using _opt versions for consistency\n",
    "val_dataset_final = TensorDataset(X_val_cnn_opt, y_val_cnn_opt)\n",
    "train_loader_final = DataLoader(train_dataset_final, batch_size=final_batch_size, shuffle=True)\n",
    "val_loader_final = DataLoader(val_dataset_final, batch_size=final_batch_size, shuffle=False)\n",
    "\n",
    "# Use your full training loop here for the final model\n",
    "num_epochs_cnn_final = 150 # Train for more epochs with best params\n",
    "best_val_acc_cnn_final = 0.0\n",
    "patience_cnn_final = 25    # Longer patience for final model\n",
    "patience_counter_cnn_final = 0\n",
    "scheduler_final = optim.lr_scheduler.ReduceLROnPlateau(final_optimizer, mode='min', patience=10, factor=0.5) # Sched for final train\n",
    "\n",
    "for epoch in range(num_epochs_cnn_final):\n",
    "    final_cnn_model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader_final:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        final_optimizer.zero_grad()\n",
    "        outputs = final_cnn_model(inputs)\n",
    "        loss = final_criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        final_optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    epoch_loss = running_loss / len(train_loader_final.dataset)\n",
    "\n",
    "    final_cnn_model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader_final:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = final_cnn_model(inputs)\n",
    "            loss_v = final_criterion(outputs, labels)\n",
    "            val_loss += loss_v.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_val_loss = val_loss / len(val_loader_final.dataset)\n",
    "    epoch_val_acc = correct / total\n",
    "    print(f\"Final CNN Train - Epoch {epoch+1}/{num_epochs_cnn_final}, Train Loss: {epoch_loss:.4f}, Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.4f}\")\n",
    "\n",
    "    scheduler_final.step(epoch_val_loss)\n",
    "\n",
    "    if epoch_val_acc > best_val_acc_cnn_final:\n",
    "        best_val_acc_cnn_final = epoch_val_acc\n",
    "        torch.save(final_cnn_model.state_dict(), 'best_tuned_cnn_1d_model.pth')\n",
    "        print(f\"New best tuned CNN val acc: {best_val_acc_cnn_final:.4f}. Model saved.\")\n",
    "        patience_counter_cnn_final = 0\n",
    "    else:\n",
    "        patience_counter_cnn_final += 1\n",
    "    \n",
    "    if patience_counter_cnn_final >= patience_cnn_final:\n",
    "        print(f\"Final CNN Early stopping at epoch {epoch+1}.\")\n",
    "        break\n",
    "\n",
    "# Load the best tuned CNN model for evaluation and prediction\n",
    "if os.path.exists('best_tuned_cnn_1d_model.pth'):\n",
    "    final_cnn_model.load_state_dict(torch.load('best_tuned_cnn_1d_model.pth'))\n",
    "    print(f\"Loaded best tuned CNN model with val acc: {best_val_acc_cnn_final:.4f}\")\n",
    "else:\n",
    "    print(\"Warning: Best tuned CNN model file not found. Using last state.\")\n",
    "\n",
    "\n",
    "models['1D-CNN (Raw TS)'] = final_cnn_model # Store the best tuned PyTorch model\n",
    "val_accuracies['1D-CNN (Raw TS)'] = best_val_acc_cnn_final\n",
    "print(f\"Best Tuned 1D-CNN Validation Accuracy: {best_val_acc_cnn_final:.4f}\")\n",
    "\n",
    "# ... (rest of your script: final predictions, summary, recommendations)\n",
    "# Ensure X_test_cnn_tensor is prepared for the final_cnn_model to predict on\n",
    "X_test_cnn_tensor = torch.tensor(X_test_ts_scaled.reshape(-1, 1, X_test_ts_scaled.shape[1]), dtype=torch.float32)\n",
    "test_dataset_cnn = TensorDataset(X_test_cnn_tensor)\n",
    "test_loader_cnn = DataLoader(test_dataset_cnn, batch_size=final_batch_size, shuffle=False) # Use final batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22f30fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best tuned CNN model with val acc: 0.7500\n",
      "Best Tuned 1D-CNN Validation Accuracy: 0.7500\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Load the best tuned CNN model for evaluation and prediction\n",
    "if os.path.exists('best_tuned_cnn_1d_model.pth'):\n",
    "    final_cnn_model.load_state_dict(torch.load('best_tuned_cnn_1d_model.pth'))\n",
    "    print(f\"Loaded best tuned CNN model with val acc: {best_val_acc_cnn_final:.4f}\")\n",
    "else:\n",
    "    print(\"Warning: Best tuned CNN model file not found. Using last state.\")\n",
    "\n",
    "\n",
    "models['1D-CNN (Raw TS)'] = final_cnn_model # Store the best tuned PyTorch model\n",
    "val_accuracies['1D-CNN (Raw TS)'] = best_val_acc_cnn_final\n",
    "print(f\"Best Tuned 1D-CNN Validation Accuracy: {best_val_acc_cnn_final:.4f}\")\n",
    "\n",
    "# ... (rest of your script: final predictions, summary, recommendations)\n",
    "# Ensure X_test_cnn_tensor is prepared for the final_cnn_model to predict on\n",
    "X_test_cnn_tensor = torch.tensor(X_test_ts_scaled.reshape(-1, 1, X_test_ts_scaled.shape[1]), dtype=torch.float32)\n",
    "test_dataset_cnn = TensorDataset(X_test_cnn_tensor)\n",
    "test_loader_cnn = DataLoader(test_dataset_cnn, batch_size=final_batch_size, shuffle=False) # Use final batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d62ac8",
   "metadata": {},
   "source": [
    "## Objective ## \n",
    "This report details the hyperparameter optimization process and results for two key models in the sleep stage classification pipeline:\n",
    "- **XGBoost Classifier** using engineered features, tuned with GridSearchCV.\n",
    "- **1D Convolutional Neural Network (CNN)** using raw scaled time series, tuned with Optuna.\n",
    "- The goal was to improve the baseline performance of these models by systematically searching for more optimal hyperparameter configurations.\n",
    "## Methodology ##\n",
    "### 1. XGBoost Hyperparameter Tuning with GridSearchCV ###\n",
    "- **Model:** XGBoost Classifier.\n",
    "- **Input Features:** Scaled engineered features (Statistical, FFT, Wavelet - 92 features).\n",
    "- **Tuning Tool:** sklearn.model_selection.GridSearchCV.\n",
    "- **Cross-Validation**: StratifiedKFold with 3 splits, ensuring class proportions were maintained in each fold. The random_state was set for reproducibility.\n",
    "Scoring Metric: accuracy.\n",
    "- **Parameter Grid Searched:**\n",
    " - n_estimators: [200, 300, 400] (Number of boosting rounds)\n",
    " - learning_rate: [0.03, 0.05, 0.1] (Step size shrinkage)\n",
    " - max_depth: [5, 7, 9] (Maximum tree depth)\n",
    " - subsample: [0.8, 0.9] (Fraction of samples used per tree)\n",
    " - colsample_bytree: [0.8, 0.9] (Fraction of features used per tree)\n",
    " - gamma: [0, 0.1] (Minimum loss reduction for a split)\n",
    " - min_child_weight: [1, 3] (Minimum sum of instance weight needed in a child)\n",
    "- **Execution:** A total of 432 hyperparameter combinations were evaluated (3 folds each, resulting in 1296 fits).\n",
    "### 2. 1D-CNN Hyperparameter Tuning with Optuna ###\n",
    "- **Model:** Custom 1D Convolutional Neural Network (PyTorch).\n",
    "- **Architecture:** 3 Conv1D layers (kernel=7, padding=3, BatchNorm, ReLU, MaxPool1D) followed by Flatten, Dropout, and 2 Dense layers. The number of filters in convolutional layers and units in the dense layer were part of the tuning.\n",
    "- **Input Features:** Raw scaled time series data.\n",
    "- **Tuning Tool:** Optuna library.\n",
    "- **Study Direction:** Maximize validation accuracy.\n",
    "- **Pruner:** MedianPruner (to stop unpromising trials early).\n",
    "Number of Trials: 30.\n",
    "- **Evaluation per Trial:** Each trial trained the CNN for up to 50 epochs with early stopping (patience=10) on the validation set.\n",
    "- **Hyperparameters Searched by Optuna:**\n",
    " - lr (learning rate): Log-uniform between 1e-4 and 1e-2.\n",
    " - weight_decay: Log-uniform between 1e-5 and 1e-2.\n",
    " - dropout_rate: Uniform between 0.2 and 0.7.\n",
    " - batch_size: Categorical from [32, 64, 128].\n",
    " - n_filters_1, n_filters_2, n_filters_3 (number of filters in conv layers):  Categorical (e.g., [32, 64], [64, 128], [128, 256]).\n",
    " - fc_units (units in the first dense layer): Categorical (e.g., [128, 256, 512]).\n",
    " - label_smoothing: Uniform between 0.0 and 0.2.\n",
    "- **Final Model Training:** After Optuna identified the best hyperparameters, the CNN was re-trained using these parameters for a potentially longer duration (150 epochs) with more patience (25 epochs) for early stopping, and ReduceLROnPlateau learning rate scheduler.\n",
    "## Results of Hyperparameter Optimization ##\n",
    "#### 1. Tuned XGBoost (GridSearchCV) ####\n",
    "- **Best Hyperparameters Found:**\n",
    " - colsample_bytree: 0.8\n",
    " - gamma: 0\n",
    " - learning_rate: 0.05\n",
    " - max_depth: 7\n",
    " - min_child_weight: 3\n",
    " - n_estimators: 300\n",
    " - subsample: 0.9\n",
    " - Best Cross-Validated Accuracy (from GridSearchCV): 0.7069\n",
    "#### 2. Tuned 1D-CNN (Optuna + Final Training) ####\n",
    "- **Best Hyperparameters Found by Optuna (Trial 2):**\n",
    " - lr: 0.001625\n",
    " - weight_decay: 0.002978\n",
    " - dropout_rate: 0.6701\n",
    " - batch_size: 64\n",
    " - n_filters_1: 32\n",
    " - n_filters_2: 128\n",
    " - n_filters_3: 128\n",
    " - fc_units: 256\n",
    " - label_smoothing: 0.1627\n",
    " - **Validation Accuracy of Best Optuna Trial (short training):** 0.7480\n",
    " - **Final Tuned 1D-CNN Validation Accuracy (after longer retraining with best params):** 0.7500\n",
    "   - This retraining process with increased epochs and patience allowed the model to converge to a slightly better performance. The model was saved at epoch 39.\n",
    "## Discussion & Conclusions ##\n",
    "- **Impact of Tuning:**\n",
    " - **XGBoost:** The GridSearchCV identified parameters that yielded a cross-validated accuracy of approximately 0.707. This provides a robust estimate of the tuned XGBoost model's performance on unseen data within the training distribution. Compare this to the initial XGBoost result (0.7010) to see if tuning provided a significant lift with this specific grid.\n",
    " - **1D-CNN:** Optuna successfully guided the search for a high-performing CNN architecture and training configuration. The best Optuna trial achieved 0.7480 accuracy. Subsequent retraining with these optimal parameters for more epochs (with early stopping) further improved the validation accuracy to 0.7500. This highlights the benefit of a two-stage process: efficient search with Optuna followed by focused training.\n",
    "- **Model Comparison Post-Tuning:** After hyperparameter optimization, the 1D-CNN (0.7500) remains the top-performing model on the validation set, followed by the tuned XGBoost (0.7069).\n",
    "- **Observations from CNN Tuning:**\n",
    " - The Optuna study explored a range of architectural (filter counts, FC units) and regularization (dropout, weight decay, label smoothing) parameters. The best-found parameters suggest that a moderately complex CNN with significant dropout and label smoothing performs well.\n",
    " - The learning rate and weight decay values found are typical for AdamW optimization.\n",
    "- **Limitations:**\n",
    " - The GridSearchCV for XGBoost was extensive but still a subset of all possible combinations.\n",
    " - The Optuna study for the CNN was limited to 30 trials; more trials could potentially uncover even better configurations.\n",
    " - The feature set for XGBoost did not include the computationally intensive ARIMA/Prophet features, which might offer further improvements if incorporated.\n",
    "## Next Steps ##\n",
    "Based on these optimization results:\n",
    "- The Tuned 1D-CNN should be considered the primary candidate for final predictions.\n",
    "-  Tuned XGBoost serves as a strong secondary model.\n",
    "- Further improvements towards the higher accuracy target should focus on:\n",
    " - **Ensemble methods:** Combining predictions from the tuned CNN and tuned XGBoost (e.g., weighted averaging or stacking).\n",
    " - **Advanced Feature Engineering/Selection:** Especially for the XGBoost model.\n",
    " - **Data Augmentation:** For the CNN to potentially improve its robustness and generalization.\n",
    " - **Deeper Error Analysis:** Investigating misclassifications of the tuned CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd47dffb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
